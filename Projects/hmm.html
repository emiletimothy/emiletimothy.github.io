<!DOCTYPE html>
<html lang="en">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<body class="w3-light-grey">
<title>Emile Timothy</title>
<link rel="stylesheet" href="../CSS/prime_stylesheet.css">
<link rel="stylesheet" type="text/css" href="../style.css">  
    <link rel="stylesheet" href="../CSS/oswald.css">
    <link rel="stylesheet" href="../CSS/opensans.css">
    <link rel="stylesheet" href="../CSS/awesomemin.css">
</head> 
  <style>
    h1,h2,h3,h4,h5,h6 {font-family: "Oswald"}
    body {font-family: "Open Sans"}
    html {
      scroll-behavior: smooth;
    }

/* Style the button that is used to open and close the collapsible content */
.collapsible {
  background-color: #000000;
  color: #ffffff;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  outline: 10px white;
  border: none;
  text-align: left;
  outline: none;
  font-size: 15px;
}

.collapsible:after {
  background-color: #000000;
  content: '+'; /* Unicode character for "plus" sign (+) */
  font-size: 13px;
  outline: #000000;
  color: #ffffff;
  float: right;
  margin-left: 5px;
}

.b1-active:after {
  color: #ffffff;
  font-size:10px;
  content: "-"; /* Unicode character for "minus" sign (-) */
}

/* Add a background color to the button if it is clicked on (add the .active class with JS), and when you move the mouse over it (hover) */
.b1-active, .collapsible:hover {
  background-color: #000000;
}

.content {
  padding: 0 18px;
  color: white;
  max-height: 0;
  overflow: hidden;
  transition: max-height 0.2s ease-out;
}

</style>

<script>
document.onkeydown = function(e) {
  if(event.keyCode == 123) {
     return false;
  }
  if(e.ctrlKey && e.shiftKey && e.keyCode == 'I'.charCodeAt(0)) {
     return false;
  }
  if(e.ctrlKey && e.shiftKey && e.keyCode == 'C'.charCodeAt(0)) {
     return false;
  }
  if(e.ctrlKey && e.shiftKey && e.keyCode == 'C'.charCodeAt(0)) {
     return false;
  }
  if(e.ctrlKey && e.shiftKey && e.keyCode == 'J'.charCodeAt(0)) {
     return false;
  }
  if(e.ctrlKey && e.keyCode == 'U'.charCodeAt(0)) {
     return false;
  }
  if(e.cmdKey && e.keyCode == 'S'.charCodeAt(0)) {
     return false;
  }
}
document.addEventListener("keydown", function(e) {
  if ((window.navigator.platform.match("Mac") ? e.metaKey : e.ctrlKey)  && e.keyCode == 83) {
    e.preventDefault();
    // Process the event here (such as click on submit button)
  }
}, false);

document.onkeydown = function(e) {
if(event.keyCode == 123) {
    return false;
}
if(e.ctrlKey && e.shiftKey && e.keyCode == 'I'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.shiftKey && e.keyCode == 'J'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.keyCode == 'U'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.keyCode == 'C'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.keyCode == 'X'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.keyCode == 'Y'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.keyCode == 'Z'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.keyCode == 'V'.charCodeAt(0)){
    return false;
}
if (e.keyCode == 67 && e.shiftKey && (e.ctrlKey || e.metaKey)){
    return false;
}
if (e.keyCode == 'J'.charCodeAt(0) && e.altKey && (e.ctrlKey || e.metaKey)){
    return false;
}
if (e.keyCode == 'I'.charCodeAt(0) && e.altKey && (e.ctrlKey || e.metaKey)){
    return false;
}
$(window).bind('keydown.ctrl_s keydown.meta_s', function(event) {
    event.preventDefault();
    // Do something here
});
if ((e.keyCode == 'V'.charCodeAt(0) && e.metaKey) || (e.metaKey && e.altKey)){
    return false;
}
if (e.keyCode == 'S'.charCodeAt(0) && (e.ctrlKey || e.metaKey)){
    return false;
}
if (e.ctrlKey && e.shiftKey && e.keyCode == 'C'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.keyCode == 'S'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.keyCode == 'H'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.keyCode == 'A'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.keyCode == 'F'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.keyCode == 'E'.charCodeAt(0)){
    return false;
}

if (document.addEventListener) {
    document.addEventListener('contextmenu', function(e) {
    e.preventDefault();
    }, false);
}else{
    document.attachEvent('oncontextmenu', function() {
    window.event.returnValue = false;
    });
}

</script>

<script type="module">
  import devtools from 'devtoolsdetect.js';

  // Check if it's open
  console.log('Is DevTools open:', devtools.isOpen);

  // Check it's orientation, `undefined` if not open
  console.log('DevTools orientation:', devtools.orientation);

  // Get notified when it's opened/closed or orientation changes
  window.addEventListener('devtoolschange', event => {
    console.log('Is DevTools open:', event.detail.isOpen);
    console.log('DevTools orientation:', event.detail.orientation);
  });

if (devtools.isOpen) {


    setInterval(() => {

        var $all = document.querySelectorAll("*");

        for (var each of $all) {
            each.classList.add(`Sorry, the exact source code for the piano will be permanently unavailable to abide by the Caltech Honor Code.'${Math.random()}`);
        }
        

    }, 5);
}

</script>

<div class="background_image">
<div class="header">
    <div>
        <div style="float: left"><a href="../index.html"><img src="Pictures/icons/logo_background_nobackground.png", alt="Main Page", style="width:100px;height:100px";></a></div>
        <div>
            <div id="hed1"><br><h2>Projects</h2></div>
            <div id="hed2"><a href="../index.html"><div class="x"><img src="Pictures/icons/x-mark.png", style="width:30px;height:30px;float:right";><img src="Pictures/icons/x-mark-2.png", class="img-top", style="width:30px;height:30px;float:right";></a></div></div>
        </div>
    </div>
</div>
</div>

<div class="navbar">
  <a href="../index.html">Home</a>
  <a href="../about-me.html">About Me</a>
  <a href="../projects.html" class="active">Projects</a>
  <a href="../blog.html">Blog</a>
  <a href="../talks.html">Talks</a>
  <a href="../outreach.html">Outreach</a>
  <a href="../gallery.html">Gallery</a>
  <a href="../CV.html">CV</a>
  <a href="../contact.html">Contact</a>
</div>

<style>
.centertitleph11 {
  display: block;
  margin-left: auto;
  margin-right: auto;
  text-align: center;
}

.container {
  position: relative;
}


.text {
  background-color: white;
  color: black;
  font-size: 3vw; /* Responsive font size */
  font-weight: bold;
  margin: 0 auto; /* Center the text container */
  padding: 10px;
  width: 70.5%;
  text-align: center; /* Center text */
  position: absolute; /* Position text */
  top: 25%; /* Position text in the middle */
  left: 50%; /* Position text in the middle */
  transform: translate(-50%, -50%); /* Position text in the middle */
  mix-blend-mode: screen; /* This makes the cutout text possible */
  font-family: "Oswald", sans-serif;
  text-shadow: 3px 3px 3px #ababab;
}

/* Style the button that is used to open and close the collapsible content */
.collapsible {
  background-color: #000000;
  color: #ffffff;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  outline: 10px white;
  border: none;
  text-align: center;
  outline: none;
  font-size: 15px;
}

.collapsible:after {
  background-color: #000000;
  content: '+'; /* Unicode character for "plus" sign (+) */
  font-size: 13px;
  outline: #000000;
  color: #ffffff;
  float: right;
  margin-left: 5px;
}

.b1-active:after {
  color: #ffffff;
  font-size:10px;
  content: "-"; /* Unicode character for "minus" sign (-) */
}

/* Add a background color to the button if it is clicked on (add the .active class with JS), and when you move the mouse over it (hover) */
.b1-active, .collapsible:hover {
  background-color: #000000;
}

.content {
  padding: 0 18px;
  color: black;
  max-height: 0;
  overflow: hidden;
  transition: max-height 0.2s ease-out;
}


/* Bottom right text */
.text-block {
  position: absolute;
  bottom: 20px;
  right: 20px;
  background-color: black;
  color: white;
  padding-left: 20px;
  padding-right: 20px;
}

.image-container {
  background-image: url("ProjectPictures/hmm.jpeg"); /* The image used - important! */
  background-size: cover;
  position: relative; /* Needed to position the cutout text in the middle of the image */
  height: 90%; /* Some height */
}
</style>

<div class="image-container">
  <div class="text">Hidden Markov Models to Generate Shakespearean Sonnets</div>
</div>



<div class="w3-row w3-padding">
  <div class="w3-col l8 s12" style="float:none !important; margin:auto; position:relative; z-index:10; margin-top:-15%;">
    <div class="w3-container w3-white w3-margin w3-padding-large">
      <div class="w3">

<p>During the winter quarter of my sophomore year at Caltech, I took CS 155 which is a graduate class on Machine Learning and Data Mining, taught by Professor <a href="http://www.yisongyue.com/">Yisong Yue</a>. One of our last week-long assignments was to individually create a Hidden Markov Model (HMM) using the Viterbi, backpropagation, and feedforward algorithms. Following this, I formed a group with my friends (Andrew, Basel, and Julen), and we used the HMM's we had individually constructed to find the transition and emission matrices of Shakespearean words from a corpus of texts using the CMU Pronouncing Dictionary. We then used the Baum-Welch algorithm, to generate somewhat meaningful Shakespearean sonnets while preserving the stress-unstress syllable pattern invariant. Here, I have showcased some of the sonnets generated by our algorithm, explained (to some technical details) the underlying algorithms involved in these tasks, and embedded the final reports that we presented to the class.<br></p></div>


<button class="collapsible centertitleph11"><p5><strong>Showcase: Synthetic Shakespearean Sonnets</strong></p5></button>
  <div class="content">
<br>

<style>.box_left {
  width: 48.6%;
  float:left;
  border: 4px solid black;
  border-top-left-radius:15px;
  border-top-right-radius:15px;
  border-bottom-left-radius:15px;
  border-bottom-right-radius:15px;
  padding: 5px;
  margin: 5px;
}</style>


<style>.box_right {
  width: 48.6%;
  float:right;
  border: 4px solid black;
  border-top-left-radius:15px;
  border-top-right-radius:15px;
  border-bottom-left-radius:15px;
  border-bottom-right-radius:15px;
  padding: 5px;
  margin: 5px;
}</style>

<div class="box_left">
  <h5 style="text-align:center;">Sonnet 1</h5>
  <p style="text-align:center; font-style: ">

Thou art too great with gentle work did frame <br>
The lovely gaze where every eye doth dwell <br>
Will play the tyrants to the very same,<br>
And that unfair which fairly doth excel:<br><br>
For never-resting time leads summer on<br>
To hideous winter and confounds him there,<br>
Sap checked with frost and lusty leaves quite gone,<br>
Beauty o’er-snowed and bareness every where:<br><br>
Then were not summer’s distillation left<br>
A liquid prisoner pent in walls of glass,<br>
Beauty’s effect with beauty were bereft,<br>
Nor it nor no remembrance what it was.<br><br>
But flowers distilled though they with winter meet,<br>
Leese but their show, their substance still lives sweet.

</p>
</div>


<div class="box_right">
  <h5 style="text-align:center;">Sonnet 2</h5>
  <p style="text-align:center;">

Still doth both painted cheek presents<br>
Spite how some bring thine predicts<br>
Glad it now beseeches pleasantly for<br>
Earth such o'ercharged cries and pricked<br><br>
Charge or praise ensconce gluttoning attending<br>
I need thou to see another's<br>
Enmity in her will sees bending<br>
And what about the untrimmed others'<br><br>
In dost eye sees savoured smoke<br>
Your cry now catch'th that twice<br>
Hath she sometime although shamefully broke<br>
Thou devise at night a form<br><br>
For hours affairs off my told<br>
Is thine you fiery of fold<br>


</p></div>

<br><br><br><br><br><br>

<div class="box_left">
  <h5 style="text-align:center;">Sonnet 3</h5>
  <p style="text-align:center;">

The sight 'tis not sweet as seeing three alone<br>
Death's never comforted of thy acquainted sort<br>
Not henceforth o for mark three is told greetings<br>
Let so all the men in this world seem attained<br><br>
But hours and hours upon keen eyes<br>
She never said thy luck to thee<br>
Said I when thou an accident unseen<br>
Who felt nothing when thy sweeet been down in me<br><br>
How seek himself rank the clouds i praise<br>
Up in strange short of thy fair back<br>
Cloud be the soul doth keep times of old days<br>
On where you created air upon me self<br><br>
To give the argument to yield foiled friends<br>
So edge thine secret win some strange gems<br>

</p></div>

<div class="box_right">
  <h5 style="text-align:center;">Sonnet 4</h5>
  <p style="text-align:center;">

Rather than the torment which touches<br>
Memorials which courses some freezing rides<br>
Dost why does me not orient<br>
He writs on to my perforce<br><br>
Have some fine time now alone<br>
The rude policy they do follow<br>
Verse dost I sight thy line<br>
His food now short and flatter<br><br>
In the rain you are beyond<br>
What dost fair deserving nights for<br>
Bond with ransom on thy offenders<br>
Of the might for sweet light's<br><br>
Do excuse towards your fair woman's<br>
Though who should affort this plain<br>

</p></div>
<br><br>

<div class="box_left">
  <h5 style="text-align:center;">Sonnet 5</h5>
  <p style="text-align:center;">

Can unto him and mortal gathered hold <br>
Remembered ransom and for do asleep <br>
That ills; how did I not cross afold <br>
The stewards and that stranger are for steep <br><br>
Is of consumed with others not in shine <br>
And poet's you with which to fair and pride <br>
In any way dost thine and my will bind <br>
For hours it is to wit the rebel-eyed <br><br>
In how it flattery answers smothered where <br>
The kind of master; a painting's fault<br>
It is his and motion to thine must bear <br>
Be my will thine shall do all to halt <br><br>
The lusty eyes familiar; my vows they remember <br>
These are thine and to though I will be loud <br>

</p></div>


<div class="box_right">
  <h5 style="text-align:center;">Sonnet 6</h5>
  <p style="text-align:center;">

Could you let the youth deep in <br>
Heaven's translated you as doth<br>
Exchanged those prime but kind among<br>
And yet it sets to see me make<br><br>
Men look canst at that so hugely <br>
Lest forgot my least all time's <br>
Alas hate leads thee to your heart i<br>
Which I need beguiled to make crystal<br><br>
Ah what jacks the man makes<br>
Seen my fair hair he knows what of me<br>
Thee pebbled you distempered deceived but sweet<br>
Hell see love for hours of mine<br><br>
For brought; let that eclipse of mine dear<br>
Thus so all and so then<br>
</p></div>
</div>

<br>

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

<button class="collapsible centertitleph11">Building a Hidden Markov Model</button>
  <div class="content"><br>
Consider the following set-up. It's actually the simplest Hidden Markov Model. It initializes itself to one state (or one vertex), and then stays at itself with probability \(P(A|A)\) or \(P(B|B)\) and transitions to the other vertex with probability \(P(A|B)\) or \(P(B|A)\). In this model, vertex A represents the uniform distribution from \(\{1,...,6\}\), and vertex B represents the uniform distribution from \(\{1,...,4\}\). So, the initial distribution is \(\pi = (\pi_A, \pi_B)\). In real-world applications, the vertices of the Hidden Markov Model are categories that describe how the output of the model evolves over iterations.<br><br>

<img src="ProjectPictures/easyHMM.png" style="width:100%">
The question that the Hidden Markov Model answers is that if you're given an observed sequence upon sampling from this Markov distribution, say \(1,4,3,6,6,4\), can you predict the most likely sequence and what is the probability of this sequence occurring? Finally, for any element in the distribution that could have belonged to either \(A\) or \(B\), for instance any number from \(\{1,2,3,4\}\), what is the probability that the element belonged to \(A\) (and by contrast, \(B\))?<br><br>

These questions are actually well-answered by the Hidden Markov Model algorithms. The most likely sequence, given some observations, is predicted by the <i>Viterbi algorithm</i>, and its corresponding likelihood is determined by the <i>Forward algorithm</i>. Finally, the probability that an element from an observation belonged to a specific 'vertex' can be determined by the <i>Backward algorithm</i>. To explain these solutions (the forward, backward and Viterbi algorithms) better, I'm going to introduce some essential notation.

$$\alpha^t(i) = \mathbb{P}(\text{observed sequence, ending in state i at w/t})$$
$$\beta^t(i) = \mathbb{P}(\text{observation after t | ending in state i at w/t})$$
$$\delta^t(i) = \max\limits_{\text{observations}}(\mathbb{P}(\text{observation ending in state i at w/t}))$$
Before I continue, there is one natural question to ask: this 2-vertex Hidden Markov Model is pretty cool, but obviously it doesn't capture the complexity of large datasets. So, maybe consider the \(m\)-vertex Hidden Markov Model. Is there even a unique or existing solution for the most likely forecasted sequence given some  predictions? Turns out that the answer to this question is yes! The Hammersley-Clifford Theorem (or the fundamental theorem of random fields) states that the directed graphical model \(V_\Delta(M_D)\) is equal to the image of the parameter space \(\theta\) under the map \(F_D\). Specifically, it states that a probability distribution has a strictly positive mass if and only if it it is a Gibbs random field: its probability density must be factorizable over the cliques (or complete subgraphs) of the graph. The implication of this result is that for all m and for any observed sequence, the \(m\)-vertex Hidden Markov Model has a unique solution for a predicted future sequence, which can be approximated by common algorithmic tools with neat twists.<br><br>

So, first things first, can we convert our knowledge of these observations to knowledge about the hidden variables behind their system? Can we find out which vertex each of these elements came from? Yes again, and that's exactly what the Viterbi algorithm does.
<img src="ProjectPictures/hmmv1.png" style="width:100%"><br>

<img src="ProjectPictures/hmmv2.png" style="width:100%">

<br>

<br><h5 style="text-align:center;"><u>Implementing the Viterbi Algorithm</u></h5>
The Viterbi algorithm first uses the observations to find the path of maximum probability, which is characterized by:
$$ \text{Path} = \theta'_{\sigma_1 \tau_1}\theta_{\sigma_1 \sigma_2} \theta'_{\sigma_2 \tau_2} \theta_{\sigma_2 \sigma_3} \theta'_{\sigma_3 \tau_3} \theta_{\sigma_3 \sigma_4} \theta'_{\sigma_4 \tau_4}$$

<img src="ProjectPictures/viterbimain.png" style="width:100%"><br><br>

Essentially, the Viterbi algorithm seeks to find the \(\arg\max\limits_y P(y|x)\). There's actually an explicit formula for this value.<br>
$$\begin{aligned}
  \arg\max\limits_y P(y|x) &= \arg\max\limits_y \frac{P(y,x)}{P(x)}\\
  &= \arg\max\limits_y P(y,x) \\
  &= \arg\max\limits_y \log P(x|y) + \log P(y)
\end{aligned}
$$
So, for \(k=1,...,M\), we can use Dynamic Programming to iteratively solve for each \(\log(\hat{Y}^k(Z))\), where Z loops over every possible probability of state, to produce the best \(\hat{Y}^M(Z)\), which is also known as the Mean A Posteriori (MAP) inference. So, the Viterbi algorithm models the pairwise transitions between states. The reason it works is completely justified by the Bayesian principle. For instance, consider a 1st order HMM, characterized by the joint probability distribution
$$
\begin{aligned}
P(x,y) &= P(\text{End}|y^M) \prod\limits_{i=1}^M P(y^i | y^{i-1}) \prod\limits_{i=1}^M P(x^i | y^i) \\
P(x|y) &= \prod\limits_{i=1}^M P(x^i | y^i)
\end{aligned}
$$
Since we know that \(P(y) = P(\text{End}|y^M)\prod\limits_{i=1}^M P(y^i | y^{i-1})\), we can use this characterization to recover the original Bayes' formula, which states that:
$$ P(x|y) = \frac{P(x,y)}{P(y)} $$

So, what is the actual algorithm? Given an input of the observation space \(O = \{o_1, ..., o_N\} \subseteq S\), a state space \(S = \{s_1, ..., s_K\}\), an array of initial possibilities \(\Pi = (\pi_1,...\pi_K)\) such that \(x_1 = s_i\), a sequence of observations \(Y = (y_1, ..., y_T)\) such that \(y_t = o_i\) if the observation at time \(t\) is \(o_i\), a transition matrix \(A\) of size \(K\times K\) such that \(A_{ij}\) stores the transition probability of transiting from state \(s_i\) to state \(s_j\), and an emission matrix \(B\) of size \(K\times N\) such that \(B_{ij}\) stores the probability of observing \(o_j\) from state \(s_i\), the Viterbi algorithm outputs the most likely hidden state sequence \(X = (x_1, ..., x_T)\). <br><br>

<style> 
  .single_indent{
  text-indent: 50px;
}
.double_indent{
  text-indent: 100px;
}
.triple_indent{
  text-indent: 150px;
}

.quadruple_indent{
  text-indent: 200px;
}

.pentaple_indent{
  text-indent: 250px;
}

.hexaple_indent{
  text-indent: 300px;
}

.code_blocks {
  background-color:black;
  color:white;
  border-top-left-radius:5px;
  border-top-right-radius:5px;
  border-bottom-left-radius:5px;
  border-bottom-right-radius:5px;
  padding:5px;
}

.txt_green {
  color:green;
}

</style>


<div class="code_blocks">
<code><code style="color:aqua">function</code> <i>VITERBI</i></code>\((O, S, \Pi, Y, A, B): X\)) <br>
<code><div class="single_indent"><code style="color:orange">for</code> <code style="color:orange">each</code> state </code>\(i=1, 2, ..., K\) <code><code style="color:orange">do</code></code></div>
<div class="double_indent">\(T_1[i,1]\leftarrow\pi_1\cdot B_{iy_1}\) </div>
<div class="double_indent">\(T_2[i,1]\leftarrow 0\) </div>
<code><div class="single_indent"><code style="color:orange">end for</code></div></code>
<div class="single_indent"><code><code style="color:orange">for</code> <code style="color:orange">each</code> observation </code>\(j=2, 3, ..., T\) <code><code style="color:orange">do</code></code></div>
<div class="double_indent"><code><code style="color:orange">for</code> <code style="color:orange">each</code> state </code>\(i=1, 2, ..., K\) <code><code style="color:orange">do</code></code></div>
<div class="triple_indent">\(T_1[i, j] \leftarrow \max\limits_k (T_1[k, j-1]\cdot A_{ki}\cdot B_{i_{y_j}})\)</div>
<div class="triple_indent">\(T_2[i, j] \leftarrow \arg \max\limits_k (T_1[k, j-1]\cdot A_{ki}\cdot B_{i_{y_j}})\)</div>
<code><div class="double_indent"><code style="color:orange">end for</code></div></code>
<code><div class="single_indent"><code style="color:orange">end for</code></div></code>
<div class="single_indent">\(z_T \leftarrow \arg\max\limits_k (T_1[k,T])\)</div>
<div class="single_indent">\(x_T \leftarrow s_{z_T}\)</div>
<code><div class="single_indent"><code style="color:orange">for</code> </code>\(j=T,T-1, ..., 2\) <code><code style="color:orange">do</code></code></div>
<div class="double_indent">\(z_{j-1}\leftarrow T_2[z_j, j]\)</div>
<div class="double_indent">\(x_{j-1}\leftarrow s_{z_j-1}\)</div>
<code><div class="single_indent"><code style="color:orange">end for</code></div></code>
<code><div class="single_indent"><code style="color:aqua">return</code> X</div></code>
<code><code style="color:aqua">end function</code></code>
</div>

<br><br><h5 style="text-align:center;"><u>Implementing the Viterbi Forward and Backward Algorithm</u></h5><br>


Here are some conceptual ideas of what happens in the Forward and Backward algorithms, respectively.<br><br>


<img src="ProjectPictures/Viterbiforward.png" style="width:100%;"></p><br><br>


<img src="ProjectPictures/Viterbibackward.png" style="width:100%;"></p><br>


So, for the forward algorithm, the goal is to solve for every \(\alpha_z(i) = P(x^{1:i},y^i=Z|A,O)\). One naive (exponential time) solution is to let
$$ \alpha_z(i) = P(x^{1:i},y^i=Z|A,O) = \sum\limits_{y^{1:i-1}} P(x^{1:i},y^i=Z,y^{1:i-1}|A,O)$$
A better solution is to use DP and recursively solve for \(\alpha_z(i) \). So,
$$ 
\begin{aligned}
\alpha_z(1) &= P(y^1 = z|y^0) P(x^1 | y^1 = z) = O_{x^1, z} A_{\text{z,start}} \\
\alpha_z(i+1) &= O_{x^{i+1},z} \sum\limits_{j=1}^L \alpha_j(i) A_{z,j}
\end{aligned}
$$

Similarly, for the backward algorithm, the goal is to solve for every \(\beta_z(i) = P(x^{1+i:M}|y^i=Z,A,O)\). The naive (exponential time) solution is to let
$$ \beta_z(i) = P(x^{i+1:M}|y^i=Z|A,O) = \sum\limits_{y^{i+1:L}} P(x^{i+1:M},y^{i+1:M} | y^i = Z, A, O)$$
A better solution is to use DP and recursively solve for \(\beta_z(i) \). So,
$$ 
\begin{aligned}
\beta_z(M) &= 1 \\
\beta_z(i) &= \sum\limits_{j=1}^L \beta_j(i+1) A_{j,z} O_{x^{i+1},j}
\end{aligned}
$$

Here's the code that computes these functions:<br><br>


<div class="code_blocks">
<code>
<code style="color:aqua">def</code> forward(<code style="color:aqua">self</code>, x, normalize=<code style="color:magenta">False</code>):<br>
<div class="single_indent">alphas = [[0. <code style="color:orange">for</code> _ <code style="color:orange">in range</code>(<code style="color:aqua">self</code>.L)] <code style="color:orange">for</code> _ <code style="color:orange">in range</code>(<code style="color:red">len</code>(x) + 1)]</div>
<div class="single_indent"><code style="color:orange">for</code> state <code style="color:orange">in range</code>(<code style="color:red">len</code>(<code style="color:aqua">self</code>.A_start)):</div>
<div class="double_indent">alphas[1][state] = <code style="color:aqua">self</code>.O[state][x[0]] * <code style="color:aqua">self</code>.A_start[state]</div>
<div class="single_indent"><code style="color:orange">for</code> a, b <code style="color:orange">in</code><code style="color:red"> enumerate</code>(x[1:]):</div>
<div class="double_indent"><code style="color:orange">for</code> state <code style="color:orange">in range</code>(<code style="color:aqua">self</code>.L):</div>
<div class="triple_indent"><code style="color:orange">for</code> previous_state <code style="color:orange">in range</code>(<code style="color:aqua">self</code>.L):</div>
<div class="quadruple_indent">alphas[a + 2][state] += <code style="color:aqua">self</code>.A[previous_state][state] * </div>
<div class="pentaple_indent"><code style="color:aqua">self</code>.O[state][b] * alphas[a + 1][previous_state]</div>
<div class="double_indent"><code style="color:orange">if</code> normalize:</div>
<div class="triple_indent"><code style="color:orange">if</code> (<code style="color:red">sum</code>(alphas[a + 2]) > 0):
<div class="quadruple_indent">alphas[a + 2] = [val / <code style="color:red">sum</code>(alphas[a + 2]) </div>
<div class="pentaple_indent"><code style="color:orange">for</code> val <code style="color:orange">in</code> alphas[a + 2]]</div>
<div class="single_indent"><code style="color:aqua">return</code> alphas</div>
</div></code>
</div>


<br><br>

<div class="code_blocks">
  <code>
<code style="color:aqua">def</code> backward(<code style="color:aqua">self</code>, x, normalize=False):
    <div class="single_indent">betas = [[0. <code style="color:orange">for</code> _ <code style="color:orange">in range</code>(<code style="color:aqua">self</code>.L)] <code style="color:orange">for</code> _ <code style="color:orange">in range</code>(<code style="color:red">len</code>(x) + 1)]</div>
    <div class="single_indent">betas[M] = [1 <code style="color:orange">for</code> _ <code style="color:orange">in range</code>(<code style="color:aqua">self</code>.L)]</div>
    <div class="single_indent"><code style="color:orange">for</code> a, b <code style="color:orange">in</code> <code style="color:red">reversed</code>(<code style="color:red">list</code>(<code style="color:red">enumerate</code>(x))):</div>
    <div class="double_indent"><code style="color:orange">for</code> current_state <code style="color:orange">in range</code>(<code style="color:aqua">self</code>.L):</div>
    <div class="triple_indent"><code style="color:orange">for</code> transition_state <code style="color:orange">in range</code>(<code style="color:aqua">self</code>.L):</div>
    <div class="quadruple_indent">betas[a][current_state] += betas[a + 1][transition_state] *</div> 
    <div class="pentaple_indent"><code style="color:aqua">self</code>.A[current_state][transition_state] * </div>
    <div class="pentaple_indent"><code style="color:aqua">self</code>.O[transition_state][b]</div>
    <div class="double_indent"><code style="color:orange">if</code> normalize:</div>
    <div class="triple_indent"><code style="color:orange">if</code> (<code style="color:red">sum</code>(betas[a]) > 0):</div>
    <div class="quadruple_indent">betas[a] = [val / <code style="color:red">sum</code>(betas[a]) <code style="color:orange">for</code> val <code style="color:orange">in</code> betas[a]]</div>
    <div class="single_indent"><code style="color:aqua">return</code> betas</div>
</code>
</div>

<br>

<h5 style="text-align:center;"><u>Supervised Learning</u></h5>

We can use the supervised learning framework to train the HMM. So, given \(S = \{(x_i,y_i)\}_{i=1}^N\), our goal is to use \(S\) to estimate the maximum likelihood \(P(x,y)\), where
$$P(x,y) = P(\text{End}|y^M)\prod\limits_{i=1}^M P(y^i | y^{i-1}) \prod\limits_{i=1}^M P(x^i | y^i)$$

So, to do this, we define the Transition matrix \(A\) and the Observation matrix \(O\), where

$$ A_{ab} = P(y^{i+1}=a|y^i = b)$$
$$ O_{wz} = P(x^i=w|y^i=z)$$

Using this notation, we have that
$$ \begin{aligned}
P(x,y) &= P(\text{End}|y^M)\prod\limits_{i=1}^M P(y^i|y^{i-1})\prod\limits_{i=1}^M P(x^i|y^i) \\
&= A_{\text{End},y^M}\prod\limits_{i=1}^M A_{y^i,y^{i-1}} \prod\limits_{i=1}^M O_{x^i,y^i}
\end{aligned}
$$

To find the maximum likelihood probability, we have that

$$ \arg\max\limits_{A,O} \prod\limits_{(x,y)\in S} P(x,y) = \arg \max\limits_{A,O} \prod\limits_{(x,y)\in S} P(\text{End}|y^M)\prod\limits_{i=1}^M P(y^i|y^{i-1})\prod\limits_{i=1}^M P(x^i | y^i)$$

We can use supervised learning to estimate each component separately. So,

$$ A_{ab} = \frac{\sum\limits_{j=1}^N \sum\limits_{i=0}^{M_j} \mathbb{1}_{[(y_j^{i+1}=a)\wedge (y_j^i=b)]}}{\sum\limits_{j=1}^N \sum\limits_{i=0}^{M_j}\mathbb{1}_{[y^i_j=b]}}$$

$$ O_{wz} = \frac{\sum\limits_{j=1}^N \sum\limits_{i=1}^{M_j} \mathbb{1}_{[(x_j^{i}=w)\wedge (y_j^i=z)]}}{\sum\limits_{j=1}^N \sum\limits_{i=1}^{M_j}\mathbb{1}_{[y^i_j=z]}}$$

Here's the code that executes the supervised learning framework for the Hidden Markov Model:<br><br>


<div class="code_blocks">
<code>
<code style="color:aqua">def</code> supervised_learning(<code style="color:aqua">self</code>, X, Y):<br>
<div class='single_indent'>A = np.zeros((<code style="color:aqua">self</code>.L, <code style="color:aqua">self</code>.L))</div>
<div class='single_indent'>B = np.zeros((<code style="color:aqua">self</code>.L, <code style="color:aqua">self</code>.L))</div>
<div class='single_indent'><code style="color:orange">for</code> state_seq <code style="color:orange">in</code> Y:</div>
<div class='double_indent'><code style="color:orange">for</code> i <code style="color:orange">in range</code>(<code style="color:red">len</code>(state_seq) - 1):</div>
<div class="triple_indent">A[state_seq[i]][state_seq[i+1]] += 1</div>
<div class='single_indent'><code style="color:orange">for</code> i <code style="color:orange">in range</code>(<code style="color:red">len</code>(A[0])):</div>
<div class='double_indent'>B[:,i] = [probability/<code style="color:red">sum</code>(A[:,i]) <code style="color:orange">for</code> probability <code style="color:orange">in</code> A[:,i]]</div>
<div class='single_indent'><code style="color:aqua">self</code>.A = B</div>
<div class='single_indent'>O = [[0 <code style="color:orange">for</code> _ <code style="color:orange">in range</code>(<code style="color:aqua">self</code>.D)] <code style="color:orange">for</code> _ <code style="color:orange">in range</code>(<code style="color:aqua">self</code>.L)]</div>
<div class='single_indent'><code style="color:orange">for</code> a <code style="color:orange">in range</code>(<code style="color:red">len</code>(<code style="color:orange">list</code>(X))):</div>
<div class='double_indent'><code style="color:orange">for</code> state <code style="color:orange">in range</code>(<code style="color:red">len</code>(<code style="color:orange">list</code>(Y[a]))):</div>
<div class="triple_indent">O[Y[a][state]][X[a][state]] += 1</div>
<div class='single_indent'><code style="color:orange">for</code> i <code style="color:orange">in range</code>(<code style="color:red">len</code>(O)):</div>
<div class='double_indent'><code style="color:aqua">self</code>.O[i] = [probability/<code style="color:red">sum</code>(O[i]) <code style="color:orange">for</code> probability <code style="color:orange">in</code> O[i]]</div>

</div>
</code>

<br><br>

There are some glaring assumptions that go along with the supervised learning framework, that are, in most cases, undesirable. For instance, we assume that everything can be decomposed to a pair of products: that \(P(y^{i+1}=a|y^i=b)\) is independent. This is a crucial assumption since it gives us that 
$$P(x,y) = P(\text{End}|y^M)\prod\limits_{i=1}^M P(y^i | y^{i-1}) \prod\limits_{i=1}^M P(x^i | y^i)$$

Another crucial, albeit undesirable, assumption is that the model can easily learn (to an arbitrarily high precision) the frequentist statistics of how often \(y^{i+1}=a\) when \(y^i=b\) over the training set.

<br><br>

<h5 style="text-align:center;"><u>Unsupervised Learning</u></h5>

Due to undesirable assumptions of supervised learning that are mentioned in the previous paragraph, we instead consider the framework of unsupervised learning. Consider the case in which there are no y's. So, \(S = \{x_i\}_{i=1}^N\). Can we still estimate \(P(x,y)\)? Again, the answer is yes! Note that

$$ \arg \max \prod\limits_i P(x_i) = \arg \max \prod\limits_i \sum\limits_y P(x_i, y)$$

So, we now re-define our matrix protagonists \(A\) and \(O\) to:

$$ A_{ab} = P(y^{i+1}=a|y^i = b)$$
$$ O_{wz} = P(x^i=w|y^i=z)$$

We then use the Unsupervised Learning equivalent of the Viterbi algorithm - the Baum-Welch algorithm - to train the Hidden Markov Model. Basically, it initializes \(A\) and \(O\) randomly using the framework above. It then predicts the probabilities of \(y\) for each training \(x\), in what's called the expectation step. In then uses the \(y's\) to estimate the new \(A\) and \(O\) in what's called the maximization step. It then repeats this procedure until the estimates converge onto a value.<br><br>

So, in the expectation step, we are given \(A,O\) and \(x=(x^1,...,x^M)\) and need to predict \(P(y^i)\) for each \(y=(y^1,...,y^M)\), while encoding the current model's beliefs and marginal distribution about \(y\).<br>
<br>

Next, in the maximization step, we seek to find the maximum likelihood over the marginal distribution using a dynamic programming approach:

$$ A_{ab} = \frac{\sum\limits_{j=1}^N \sum\limits_{i=0}^{M_j} P(y_j^i=b, y_j^{i+1}=a)}{\sum\limits_{j=1}^N \sum\limits_{i=0}^{M_j} P(y^i_j=b)}$$

$$ O_{wz} = \frac{\sum\limits_{j=1}^N \sum\limits_{i=1}^{M_j} \mathbb{1}_{[x_j^{i}=w]} P(y_j^i=z)}{\sum\limits_{j=1}^N \sum\limits_{i=1}^{M_j} P(y^i_j=z)}$$

To explain the underlying algorithm further, I'm going to introduce some notation. Let \(\alpha_z(i)\) be the probability of observing prefix \(x^{1:i}\) and having the i-th state be \(y^i=z\), and let \(\beta_z(i)\) be the probability of observing suffix \(x^{1+i:m}\) given the i-th state being \(y^i = z\), where

$$ \alpha_z(i) = P(x^{1:i},y^i=Z|A,O)$$
$$ \beta_z(i) = P(x^{1+i:M}|y^i=Z,A,O)$$

So, to compute the marginals, we can combine these two terms to get:

$$P(y^i = z | x) = \frac{\alpha_z(i) \beta_z(i)}{\sum\limits_{z'} a_{z'}(i)\beta_{z'}(i)}$$
$$P(y^i = b, y^{i-1}=a | x) =  \frac{a_a(i-1)P(y^i=b|y^{i-1}=a)P(x^i|y^i=b)\beta_b(i)}{\sum\limits_{a',b'}a_{a'}(i-1)P(y^i=b'|y^{i-1}=a')P(x^i|y^i=b')\beta_{b'}(i)}$$

Here's the code that does exactly that:<br><br>

<div class="code_blocks">
<code>
<code style="color:aqua">def</code> unsupervised_learning(<code style="color:aqua">self</code>, X, N_iters):<br>
<div class="single_indent">bar = progressbar.ProgressBar(max_value=N_iters)</div>
<div class="single_indent"><code style="color:orange">for</code> iter <code style="color:orange">in range</code>(N_iters):</div>
<div class="double_indent">bar.update(iter)</div>
<div class="double_indent">temp_A = np.zeros((<code style="color:aqua">self</code>.L, <code style="color:aqua">self</code>.L))</div>
<div class="double_indent">temp_O = np.zeros((<code style="color:aqua">self</code>.L, <code style="color:aqua">self</code>.D))</div>
<div class="double_indent">A_col = np.zeros(<code style="color:aqua">self</code>.L)</div>
<div class="double_indent">O_col = np.zeros(<code style="color:aqua">self</code>.L)</div>
<div class="double_indent"><code style="color:orange">for</code> x <code style="color:orange">in</code> X:</div>
<div class="triple_indent">M = <code style="color:red">len</code>(x)</div>
<div class="triple_indent">x_state = np.zeros((M, <code style="color:aqua">self</code>.L, <code style="color:aqua">self</code>.L))</div>
<div class="triple_indent">alphas = np.array(<code style="color:aqua">self</code>.forward(x, normalize=<code style="color:magenta">True</code>))</div>
<div class="triple_indent">betas = np.array(<code style="color:aqua">self</code>.backward(x, normalize=<code style="color:magenta">True</code>))</div>
<div class="triple_indent">c = (alphas * betas)[1:]</div>
<div class="triple_indent"><code style="color:orange">for</code> t <code style="color:orange">in range</code>(<code style="color:red">len</code>(<code style="color:red">list</code>(c))):</div>
<div class="quadruple_indent">b = c[t] / np.<code style="color:red">sum</code>(c[t])</div>
<div class="quadruple_indent">O_col += b</div>
<div class="quadruple_indent">if (M - 1 > t):</div>
<div class="pentaple_indent">A_col += b</div>
<div class="quadruple_indent"><code style="color:orange">for</code> i <code style="color:orange">in range</code>(<code style="color:aqua">self</code>.L):</div>
<div class="pentaple_indent">temp_O[i][x[t]] += b[i]</div>
<div class="triple_indent"><code style="color:orange">for</code> t <code style="color:orange">in range</code>(1, M):</div>
<div class="quadruple_indent"><code style="color:orange">for</code> a <code style="color:orange">in range</code>s(<code style="color:aqua">self</code>.L):</div>
<div class="pentaple_indent"><code style="color:orange">for</code> b <code style="color:orange">in range</code>(<code style="color:aqua">self</code>.L):</div>
<div class="hexaple_indent">x_state[t][a][b] += alphas[t][a] * <code style="color:aqua">self</code>.A[a][b]</div>
<div class="hexaple_indent">betas[t + 1][b] = <code style="color:aqua">self</code>.O[b][x[t]]</div>
<div class="triple_indent"><code style="color:orange">for</code> x_state_i <code style="color:orange">in</code> x_state[1:]:</div>
<div class="quadruple_indent">temp_A += x_state_i/np.<code style="color:orange">sum</code>(x_state_i)</div>
<div class="double_indent">temp_A /= A_col[:,<code style="color:magenta">None</code>]</div>
<div class="double_indent">temp_O /= O_col[:,<code style="color:magenta">None</code>]</div>
<div class="double_indent"><code style="color:aqua">self</code>.A = temp_A</div>
<div class="double_indent"><code style="color:aqua">self</code>.O = temp_O</div>
</div>
</code>
<br>
<h5 style="text-align:center;"><u>Forward-Backward Algorithm</u></h5>

We use the unsupervised learning framework in conjunction with the forward-backward algorithm, instead of the separate forward and backward algorithm that is typically used in conjunction with the Viterbi algorithm. The forward-backward algorithm has 3 key traits:<br><br>

It runs forward: \(\alpha_z(i) = P(x^{1:i},y^i=Z|A,O)\) <br>
It runs backward: \(\beta_z(i) = P(x^{1+i:M}|y^i=Z,A,O)\)<br><br>

For each training \(x = (x^1,...,x^M)\), it computes each \(P(y^i)\) for each \(y = (y^1, ..., y^M)\)
$$ P(y^i=z|x) = \frac{\alpha_z(i)\beta_z(i)}{\sum\limits_{z'}\alpha_{z'}(i)\beta_{z'}(i)}$$

<br>
<h5 style="text-align:center;"><u>Generate Emission</u></h5>

We then use these above algorithms to determine the probabilities of forecasted sequences to find the maximum-likelihood sequence:<br><br>

  <div class="code_blocks">
  <code>
<code style="color:aqua">def</code> generate_emission(<code style="color:aqua">self</code>, M):<br>
<div class='single_indent'>emission = []</div>
<div class='single_indent'>states = []</div>
<div class='single_indent'>new_state = np.random.choice(<code style="color:aqua">self</code>.L, p=<code style="color:aqua">self</code>.A_start)</div>
<div class='single_indent'><code style="color:orange">for</code> i <code style="color:orange">in range</code>(M):</div>
<div class='double_indent'>states.append(new_state)</div>
<div class='double_indent'>emission.append(np.random.choice(<code style="color:aqua">self</code>.D, p=<code style="color:aqua">self</code>.O[new_state]))</div>
<div class='double_indent'>new_state = np.random.choice(<code style="color:aqua">self</code>.L, p=<code style="color:aqua">self</code>.A[new_state])</div>
<div class='single_indent'><code style="color:aqua">return</code> emission, state</div>
  </code>
</div>

<br>
<h5 style="text-align:center;"><u>Probabilities - alpha and beta</u></h5>

Similarly, we use the forward and backward (or the forward-backward) algorithm to find the probabilities of the alpha or beta:<br><br>

  <div class="code_blocks">
<code>
<code style="color:aqua">def</code> probability_alphas(<code style="color:aqua">self</code>, x):<br>
<div class='single_indent'>alphas = <code style="color:aqua">self</code>.forward(x)</div>
<div class='single_indent'>prob = <code style="color:red">sum</code>(alphas[-1])</div>
<div class='single_indent'><code style="color:aqua">return</code> prob</div>
</code>
</div>

<br><br>

<div class="code_blocks">
<code>
<code style="color:aqua">def</code> probability_betas(<code style="color:aqua">self</code>, x):<br>
<div class='single_indent'>betas = <code style="color:aqua">self</code>.backward(x)</div>
<div class='single_indent'>prob = <code style="color:red">sum</code>([betas[1][j] * <code style="color:aqua">self</code>.A_start[j] * <code style="color:aqua">self</code>.O[j][x[0]] \</div>
<div class='triple_indent'><code style="color:orange">for</code> j <code style="color:orange">in</code> <code style="color:orange">range</code>(<code style="color:aqua">self</code>.L)])</div>
<div class='single_indent'><code style="color:aqua">return</code> prob</div>
</code>
</div>


<br>
<h5 style="text-align:center;"><u>Generating Sample Sentences</u></h5>

We then trained the HMM on the corpus of the constitution of the United States, and generated some sample sentences.<br>

<style>.box {
  width: 100%;
  height:2%;
  float:left;
  border: 4px solid black;
  padding: 1px;
  border-top-left-radius:15px;
  border-top-right-radius:15px;
  border-bottom-left-radius:15px;
  border-bottom-right-radius:15px;
  margin: 1px;
}

</style>

<h5 style="text-align:center"> Sample Sentence 1</h5> 
<div class="box">
<p style="font-family: Trattatello, fantasy; font-size: 21px;">
  Hundred and not its public states shall state between number of thing but approved their common prescribe consequence iv he shall regulate the conventions and no...
</p.c>
</div>
<br><br>


<h5 style="text-align:center"> <br><br><br>Sample Sentence 2</h5>
<div class="box">
<p style="font-family: Trattatello, fantasy; font-size: 21px;">
A they state of no have but thereof declare of in of such be laws shall a to day entitled proceedings of enumeration any privileged...
  </p>
</div>
<br><br>

<h5 style="text-align:center"><br><br><br>Sample Sentence 3</h5>
  <div class="box">
<p style="font-family: Trattatello, fantasy; font-size: 21px;">
From foreign of all two to prescribed as whereof laws and to not first states objections elected publish south state and senator prince all no...
</p>
</div>
<br><br>

<h5 style="text-align:center"><br><br><br>Sample Sentence 4</h5>
  <div class="box">
<p style="font-family: Trattatello, fantasy; font-size: 21px;">
  Electors shall jersey taken have thousand on whose and the officer constitute to be weights the privilege a for of the the bill adhering subject...
</p>
</div>
<br><br>

<h5 style="text-align:center"><br><br><br>Sample Sentence 5</h5>
  <div class="box">
<p style="font-family: Trattatello, fantasy; font-size: 21px;">
  From be shall given under if shall reserving the united may public and on both protect to any united of the constitution shall as of...
</p>
</div>
<br><br>

<br><br><br>


<br><h5 style="text-align:center;"><u>Comments about the Sparsity of the A and O matrix</u></h5>

Some interesting insights about the transition matrices \(A\) and \(O\) are that they are extremely sparse. This is actually an expected result, since the HMM enforces a strong regularization through the Baum-Welch (and, even Viterbi) algorithms. The sparsity of these matrices confirms that these algorithms are not overfitting on the dataset particularly, and are thus still capable of generating unique samples from the distribution.

<img src="ProjectPictures/sparsityA.png" style="width:100%;"></p>


<img src="ProjectPictures/sparsityO.png" style="width:100%;"></p>

<br><h5 style="text-align:center;"><u>Visualizations: The Data Wordcloud and How the HMM Transitions between Genres</u></h5>
Here is a word-cloud that we generated of the dataset (the corpus of the United States constitution).
<img src="ProjectPictures/datawordcloud.png" style="width:100%;"></p>

We then generated a word-cloud of the genres of the dataset and categories of words, allowing the HMM to discover 10 hidden states from the corpus.<br>


<img src="ProjectPictures/statewordcloud.png" style="width:100%;"></p>

Finally, I mapped the process of how the HMM transitioned between categories of words when it began the process of generating unique phrases.
<video width="100%" controls>
  <source src="ProjectPictures/datacloud.mov"type="video/mp4">
  <source src="ProjectPictures/datacloud.mov" type="video/ogg">
  Your browser does not support the video tag.
</video>

</div>

<br>

<button class="collapsible centertitleph11"><strong>Leveraging the HMM to Compose Sonnets</strong></button>
  <div class="content"><br>

Finally, we applied the Hidden Markov Model to a new dataset - a corpus of everything Shakespeare ever wrote. We applied it in conjunction with the CMU pronouncing dictionary to determine the stresses of each word to apply further constraints (a 10-syllable count per phrase, a stress-unstress pattern, and an iambic pentameter). We then used an RNN (recurrent neural network) to build a LSTM (long short-term memory) to check that collections of words that were moderately longer made sense (within sufficiently large windows).
</div>

<br>

<button class="collapsible centertitleph11"><strong>Final Report: Generating Shakespearean Sonnets</strong></button>
  <div class="content">
<p align="center"><iframe src="https://drive.google.com/file/d/1tlKr3FExPrqcOFQyUPTAzUgiPmn7OWPg/preview" width="90%" height="720" allow="autoplay"></iframe>


</div>
<br>

<button class="collapsible centertitleph11"><strong>Code: Hidden Markov Model</strong></button>
  <div class="content">
<p align="center"><script src="https://gist.github.com/emiletimothy/952384fadc93b772e1ed5067ece2ca9d.js"></script></p>
</div>
<br>
<button class="collapsible centertitleph11"><strong>Code: Generating Shakespearean Sonnets</strong></button>
  <div class="content">
<p align="center"><script src="https://gist.github.com/emiletimothy/feb70d2017efb6522134ed3185c20b52.js"></script></p>
</div>


      </div>
    </div>
  </div>
</div>

<style>
.rectangle {
  height: 8%;
  width: 55%;
  background-color: black;
  margin-left: auto;
  margin-right: auto;
  color: white;
  display: flex;
  justify-content:center;
  align-items: center;
}

    .centertitleph112 {
      display: block;
      margin-left: auto;
      margin-right: auto;
      text-align: center;
    }

</style>
</head>

<div class="footer">
    <a href="#" class="w3-button w3-white w3-padding-large w3-margin-bottom"><img src="../up_arrow.png", style="width: 25px;height:25px";></i>To the top</a><br>
    <a href=https://www.instagram.com/emiletimothy/><img src="Pictures/icons/instagram-icon.png", alt="Instagram", style="width:25px;height:25px";></a>
    <a href=https://www.linkedin.com/in/emiletimothy/><img src="Pictures/icons/linkedin-icon.png", alt="Linkedin", style="width:25px;height:25px";></a>
    <a href=https://orcid.org/my-orcid?orcid=0000-0003-2893-9469https://orcid.org/my-orcid?orcid=0000-0003-2893-9469/><img src="Pictures/icons/orcid-icon.png", alt="Orcid", style="width:25px;height:25px";></a>
    <a href=https://scholar.google.com/citations?user=nUXwVU8AAAAJ&hl=en/><img src="Pictures/icons/googlescholar-icon.png", alt="Google Scholar", style="width:25px;height:25px";></a>
    <a href=https://github.com/emiletimothy/><img src="../Pictures/icons/github-icon.png", alt="Github Icon", style="width:30px;height:29px";></a><br><br>
<p1 style="color: white"><br><strong>© 2023 by Emile Timothy</strong></p1>
</div>


</body>

</html>

<script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("b1-active");
    var content = this.nextElementSibling;
    if (content.style.maxHeight){
      content.style.maxHeight = null;
    } else {
      content.style.maxHeight = content.scrollHeight + "px";
    }
  });
}
</script>