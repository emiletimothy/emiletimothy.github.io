<!DOCTYPE html>
<html lang="en">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<body class="w3-light-grey" oncontextmenu="return false">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>


<script>
document.onkeydown = function(e) {
  if(event.keyCode == 123) {
     return false;
  }
  if(e.ctrlKey && e.shiftKey && e.keyCode == 'I'.charCodeAt(0)) {
     return false;
  }
  if(e.ctrlKey && e.shiftKey && e.keyCode == 'C'.charCodeAt(0)) {
     return false;
  }
  if(e.ctrlKey && e.shiftKey && e.keyCode == 'C'.charCodeAt(0)) {
     return false;
  }
  if(e.ctrlKey && e.shiftKey && e.keyCode == 'J'.charCodeAt(0)) {
     return false;
  }
  if(e.ctrlKey && e.keyCode == 'U'.charCodeAt(0)) {
     return false;
  }
  if(e.cmdKey && e.keyCode == 'S'.charCodeAt(0)) {
     return false;
  }
}
document.addEventListener("keydown", function(e) {
  if ((window.navigator.platform.match("Mac") ? e.metaKey : e.ctrlKey)  && e.keyCode == 83) {
    e.preventDefault();
    // Process the event here (such as click on submit button)
  }
}, false);

document.onkeydown = function(e) {
if(event.keyCode == 123) {
    return false;
}
if(e.ctrlKey && e.shiftKey && e.keyCode == 'I'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.shiftKey && e.keyCode == 'J'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.keyCode == 'U'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.keyCode == 'C'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.keyCode == 'X'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.keyCode == 'Y'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.keyCode == 'Z'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.keyCode == 'V'.charCodeAt(0)){
    return false;
}
if (e.keyCode == 67 && e.shiftKey && (e.ctrlKey || e.metaKey)){
    return false;
}
if (e.keyCode == 'J'.charCodeAt(0) && e.altKey && (e.ctrlKey || e.metaKey)){
    return false;
}
if (e.keyCode == 'I'.charCodeAt(0) && e.altKey && (e.ctrlKey || e.metaKey)){
    return false;
}
$(window).bind('keydown.ctrl_s keydown.meta_s', function(event) {
    event.preventDefault();
    // Do something here
});
if ((e.keyCode == 'V'.charCodeAt(0) && e.metaKey) || (e.metaKey && e.altKey)){
    return false;
}
if (e.keyCode == 'S'.charCodeAt(0) && (e.ctrlKey || e.metaKey)){
    return false;
}
if (e.ctrlKey && e.shiftKey && e.keyCode == 'C'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.keyCode == 'S'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.keyCode == 'H'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.keyCode == 'A'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.keyCode == 'F'.charCodeAt(0)){
    return false;
}
if(e.ctrlKey && e.keyCode == 'E'.charCodeAt(0)){
    return false;
}

if (document.addEventListener) {
    document.addEventListener('contextmenu', function(e) {
    e.preventDefault();
    }, false);
}else{
    document.attachEvent('oncontextmenu', function() {
    window.event.returnValue = false;
    });
}

</script>

<script type="module">
  import devtools from 'devtoolsdetect.js';

  // Check if it's open
  console.log('Is DevTools open:', devtools.isOpen);

  // Check it's orientation, `undefined` if not open
  console.log('DevTools orientation:', devtools.orientation);

  // Get notified when it's opened/closed or orientation changes
  window.addEventListener('devtoolschange', event => {
    console.log('Is DevTools open:', event.detail.isOpen);
    console.log('DevTools orientation:', event.detail.orientation);
  });

if (devtools.isOpen) {


    setInterval(() => {

        var $all = document.querySelectorAll("*");

        for (var each of $all) {
            each.classList.add(`Sorry, this source code is unavailable.'${Math.random()}`);
        }
        

    }, 5);
}

</script>


<title>Emile Timothy</title>
<link rel="stylesheet" href="../CSS/prime_stylesheet.css">
<link rel="stylesheet" type="text/css" href="../style.css">  
    <link rel="stylesheet" href="../CSS/oswald.css">
    <link rel="stylesheet" href="../CSS/opensans.css">
    <link rel="stylesheet" href="../CSS/awesomemin.css">
</head> 
  <style>
    h1,h2,h3,h4,h5,h6 {font-family: "Oswald"}
    body {font-family: "Open Sans"}
    html {
      scroll-behavior: smooth;
    }
    </style>

<div class="background_image">
<div class="header">
    <div>
        <div style="float: left"><a href="../index.html"><img src="Pictures/icons/logo_background_nobackground.png", alt="Main Page", style="width:100px;height:100px";></a></div>
        <div>
            <div id="hed1"><br><h2>Blog</h2></div>
            <div id="hed2"><a href="../index.html"><div class="x"><img src="Pictures/icons/x-mark.png", style="width:30px;height:30px;float:right";><img src="Pictures/icons/x-mark-2.png", class="img-top", style="width:30px;height:30px;float:right";></a></div></div>
        </div>
    </div>
</div>
</div>

<div class="navbar">
  <a href="../index.html">Home</a>
  <a href="../about-me.html">About Me</a>
  <a href="../projects.html">Projects</a>
  <a href="../blog.html" class="active">Blog</a>
  <a href="../talks.html">Talks</a>
  <a href="../outreach.html">Outreach</a>
  <a href="../gallery.html">Gallery</a>
  <a href="../CV.html">CV</a>
  <a href="../contact.html">Contact</a>
</div>

<style>
.centertitleph11 {
  display: block;
  margin-left: auto;
  margin-right: auto;
  text-align: center;
}

.container {
  position: relative;
}


.text {
  background-color: white;
  color: black;
  font-size: 3vw; /* Responsive font size */
  font-weight: bold;
  margin: 0 auto; /* Center the text container */
  padding: 10px;
  width: 84%;
  text-align: center; /* Center text */
  position: absolute; /* Position text */
  top: 25%; /* Position text in the middle */
  left: 50%; /* Position text in the middle */
  transform: translate(-50%, -50%); /* Position text in the middle */
  /*mix-blend-mode: screen;  This makes the cutout text possible */
  font-family: "Oswald", sans-serif;
  text-shadow: 3px 3px 3px #ababab;
}


/* Bottom right text */
.text-block {
  position: absolute;
  bottom: 20px;
  right: 20px;
  background-color: black;
  color: white;
  padding-left: 20px;
  padding-right: 20px;
}

  .single_indent{
  text-indent: 50px;
}
.double_indent{
  text-indent: 100px;
}
.triple_indent{
  text-indent: 150px;
}

.quadruple_indent{
  text-indent: 200px;
}

.pentaple_indent{
  text-indent: 250px;
}

.hexaple_indent{
  text-indent: 300px;
}

.image-container {
  background-image: url("blog_pics/ecc.webp"); /* The image used - important! */
  background-size: cover;
  position: relative; /* Needed to position the cutout text in the middle of the image */
  height: 80%; /* Some height */
}
</style>

<div class="image-container">
  <div class="text">List Decoding of the Hadamard, Reed Solomon, and Parvaresh-Vardy Codes
</div>
</div>




<div class="w3-row w3-padding">
  <div class="w3-col l9 s12" style="float:none !important; margin:auto; position:relative; z-index:10; margin-top:-10%;">
    <div class="w3-container w3-white w3-margin w3-padding-large">
      <div class="w3">
<p> 
Hi everyone! In this post, I would like to describe some error-correcting codes, and talk about the notion of unique decoding versus list decoding, and I'm going to do this over several examples of error correcting codes (namely, the binary Hadamard code, Reed Solomon code, Low-Density Parity-Check code, and Parvaresh-Vardy codes). The point of these examples is to drive in the usefulness of list decodings and to talk about the really cool results that have materialized in this field over the past few years.
</p>
      </div>


<button class="collapsible"><strong>List Decoding versus Unique Decoding</strong></button>
  <div class="content">
<br>
An error-correcting code is a code that is used to fix errors in data that is transmitted over an unreliable channel. The concept was pioneered in the 1940s by the American Mathematician Richard Hamming with his invention of the Hamming (7, 4) code. Here is an intuitive geometric idea for how they work: consider a bunch of \(\mathbb{R}^n\)-spheres spread out in an n-dimensional space, where each sphere has a pretty well-defined center. The idea here is that the center of each sphere is a code-word. If this code-word gets distorted through the introduction of noise in the channel, one would expect a few of these bits to get flipped. The radius of each sphere is the maximum number of bits that can be flipped during transmission in this noisy channel (in other words, the number of incorrect bits) from where you can still deduce that the original message was actually the codeword that corresponds to the center of the sphere. So, at the receiving channel, any message that is contained within a sphere is mapped to its corresponding code-word. There are some pretty strong theoretical guarantees that these code-words can correct errors in messages with high probability. After all, in extremely noisy mediums, a word could become so distorted that it becomes another word entirely, and these level of catastrophic errors are truly unfixable at the receiving-side.<br><br>

One thing to note is that the procedure (in all its abstractness, so far) truly describes a method to deduce a unique deterministic word from every message, which could have a high-probability of accuracy depending on the code-word used. This procedure is referred to as a unique decoding. However, in extreme cases it could be susceptible to catastrophic errors with a non-zero probability. To account for these cases, it is generally preferable to instead have an algorithm that outputs a vector with the most likely code-words (arranged in order of highest-probability to lowest-probability). List-decoding accomplishes exactly this - it takes a codeword at the receiver-side and extrapolates a list of the possible messages it might have referred to.


</div><br>

<button class="collapsible"><strong>List Decoding of the Binary Hadamard Code</strong></button>
  <div class="content">
<br>
Throughout this problem \(\mathbb{F}_2\) is the field with \(2\) elements (addition and multiplication are performed modulo \(2\)). Given a \(k\)-bit message \(m\), the associated Hadamard codeword \(C(m)\) is described by first producing a linear multivariate polynomial $$p_m(x_0, x_1, ..., x_{k-1}) = \sum\limits_{i=0}^{k-1} m_i x_i,$$ and then evaluating that polynomial at all vectors in the space \(\mathbb{F}_2^k:C(m)=(p_m(w))_{w\in\mathbb{F}_2^k}\). Thus the codeword has \(n = 2^k\) bits and the \(w\)-th bit is the inner product mod \(2\) of the \(k\)-bit vectors \(m\) and \(w\). The bits of a codeword \(C = C(m)\) are naturally indexed by \(\mathbb{F}_2^k\); we write \(C_w\) (with \(w\in\mathbb{F}_2^k\) to mean the \(w\)-th coordinate, which is just \(p_m(w)\). Since the distance of the Hadamard code is \((1/2)n\) (by the Schwartz-Zippel lemma which is a well-known result in theoretical computer science), unique decoding is only possible from up to \((1/4)n\) errors. My goal for this section is to show several things that culminate in the idea that efficient list-decoding is possible from a received word \(R\) that has suffered upto \((\frac{1}{2}-\epsilon)n\) errors, and to ultimately generalize this to a really strong statement in the form of the Goldreich-Levin theorem.<br><br>

Firstly, consider the following probabilistic procedure. Pick \(\ell\) vectors \(v_1, v_2, ..., v_\ell \in \mathbb{F}_2^k\) independently and uniformly at random. For a subset \(S \subseteq \{1, 2, 3, ... \ell\}\) define \(u_S = \sum_{𝒊\in S} v_i\). Firstly, I'm going to show that for every non- empty set \(S\) and every \(\alpha\in\mathbb{F}_2^k\), we have \(\Pr[u_s=\alpha]=2^{-k}\). Secondly, I'm going to show that for every pair of non-empty subsets \(S\) and \(T\) with \(S \neq T\), and every \(\alpha, \beta \in \mathbb{F}_2^k\), we
have \(\Pr[u_s=\alpha \wedge u_t=\beta] = \Pr[u_s=\alpha] \Pr[u_t = \beta]\). In other words, the set of vectors \(u_S\) are pairwise independent random variables uniformly distributed on \(\mathbb{F}_2^k\).<br><br>

<strong>Lemma 1: For every non-empty set \(S\) and every \(\alpha \in \mathbb{F}_2^k , \Pr[u_s = \alpha] = 2^{-k}.\)</strong>
<div class="single_indent">To prove this, we derive inspiration from the principle of completing the Markov chain.</div>
<div class="single_indent">For every non-empty set \(S\subseteq \{1, 2, 3, ..., \ell\}\), let the last element in the chain from \(s\) be \(p\). It follows then that:</div>

$$u_s = \alpha \Rightarrow \alpha = v_p + \sum\limits_{i\in S - \{p\}} v_i$$

<div class="single_indent">Then, if \(v_p\) is the last vector chosen (whatever \(p\) might be, so the process is i.i.d.), what is the probability of:</div>

$$v_p = \alpha - \sum\limits_{i\in S-\{p\}} v_i$$

<div class="single_indent">Then, since the other \(v_i\) were chosen independently, we just need to find \(\Pr[\alpha=c], c = (\alpha=v_p+\sum_{i\in S-\{p\}} v_i)\)</div>
<div class="single_indent">Here, \(c \mod k \in \mathbb{F}_2^k\). Since there are \(k\) digits of \(\alpha\), the probability that the bitwise subtraction fits the criteria for 1-bit is \(\frac{1}{2}\).</div>

<div class="single_indent">Across all bits, the probability is the product of the probability that each vector in \(\mathbb{F}_2^k\) fits the criteria (the vectors are iid).</div>
<div class="single_indent">Therefore, </div>
$$ \Pr[u_s=\alpha]=\prod\limits_{i=1}^k \frac{1}{2} = 2^{-k}$$
<div class="single_indent">QED.</div><br>


<strong>Lemma 2: For every pair of non-empty subsets \(S\) and \(T\) with \(S \neq T\), and every \(\alpha, \beta \in \mathbb{F}_2^k\), we have:
$$\Pr[u_S = \alpha \wedge u_T = \beta] = \Pr[u_S = \alpha]\Pr[u_T = \beta]$$
</strong>
<div class="single_indent">Given an arbitrary pair of distinct non-empty subsets \(S\) and \(T\), we ‘complete the Markov chain’ as in lemma 1. </div>
<div class="single_indent">Let \(p\in S, p\notin T, q\in T, q\in S\) such that:</div>

$$\alpha = v_p + \sum\limits_{i\in S-\{p\}} v_i$$

$$\beta = v_q + \sum\limits_{i\in T-\{q\}} v_i$$

<div class="single_indent">(Since \(S\neq T\), \(p\) and \(q\) must exist).</div>

<div class="single_indent">Then, since \(\exists v_p, v_q \in \mathbb{F}_2^q\) and since the vectors \(v_i\) are chosen independently, we use lemma 1 to write that:</div>

$$\Pr[u_S = \alpha \wedge u_T = \beta] = \Pr[u_S = \alpha]\Pr[u_T = \beta]$$

$$ = 2^{−𝑘} \times 2^{−k} = 2^{−2k} $$

<div class="single_indent">Hence, $$\Pr[u_S = \alpha \wedge u_T = \beta] = \Pr[u_S = \alpha]\Pr[u_T = \beta]$$</div>
<div class="single_indent">QED.</div><br><br>


Suppose then that the received word \(R\) agrees with \(C = C(m)\) in at least a \(\frac{1}{2}+\epsilon\) fraction of its \(n\) bits. We can then observe that \(C_w + C_{w+e_i} = m_i\) (here, \(i\) is the \(i\)-th elementary vector in \(\mathbb{F}_2^k\) – the vector with \(1\) in the \(i\)-th position and zeros elsewhere). Of course, in our decoding algorithm, we do not have access to \(C\) to find \(C_w\) and \(C_{w+e_i}\). So, we will replace \(C_w\) with a “guess” (for now imagine it is always correct), and \(C_{w+e_i}\) with \(R_{w+e_i}\) (which may or may not be correct). We can then show that for all i, that:

$$ \Pr[|\{S\neq\Phi: C_{u_s}+R_{u_s+e_i}=m_i\}|\leq \frac{2^\ell-1}{2}] \leq \frac{1}{4\epsilon^2 (2^\ell-1)}$$

Let a family of indicator random variables for the event that \(R_{u_s+e_i} = C_{u_s+e_i}\) be \(L_s\) such that \(L_s\) be the number of bits for which the equality is met for the expression (on that particular value of \(S\)). Then, (rewriting the LHS of the probability statement using the indicator class) let \(L = \Sigma L_s\) such that \(S = \phi\). It follows then that:

$$ |\{S\neq\phi: C_{u_S}+R_{u_S+e_i} = m_i\}|=L$$

implies that

$$\Pr[|\{S\neq\phi: C_{u_S} + R_{u_s+e_i} = m_i\}| \leq \frac{2^\ell - 1}{2} = \Pr[𝐿 \leq \frac{2^\ell-1}{2}]$$

To use Chebyshev’s inequality, we now have to determine \(\mathbb{E}[L]\), \(\mathbb{V}ar[L]\) which we will do from 
\(\mathbb{E}[L_S], \mathbb{V}ar[L_S]\):

Since the received word \(R\) agrees with \(C = C(m)\) in at least a \(\frac{1}{2} + \epsilon\) fraction of its \(n\) bits (by assumption), we have that:

$$\mathbb{R}[L_S] ≥ \frac{1}{2} + \epsilon$$

Therefore, by the linearity of the expectation formula:

$$\mathbb{E}[L]=(2\ell −1)\mathbb{E}[L_S] \geq (\frac{1}{2}+\epsilon)(2^\ell −1)$$

Using the variance formula:

$$\mathbb{V}ar[L_S] = \mathbb{E}[L_S^2] − \mathbb{E}[L_S]^2$$

$$ = \sum\limits_S L_s^2 \Pr[R_{u_s+e_i}=C_{u_s+e_i}] - (\frac{1}{4}-\epsilon^2-\epsilon)_>$$

$$ = \frac{1}{2} - (\frac{1}{4} - \epsilon^2 - \epsilon)_> \leq \frac{1}{4}$$

$$ \mathbb{V}ar[L_S] \leq \frac{1}{4}$$

Then, using that the variance of the sum is equal to the sum of the variance (since \(L_S\) is pairwise independent) and since the length of the message composed from set \(S\subseteq \{1,2,3,...,\ell\}\) is \(2^\ell-1\), we have that  (by the linearity of the variance for covariant distributions from separate random independent variables):

$$ \mathbb{V}ar[L] \leq \frac{2^\ell-1}{4}$$

(A brief sanity check: Note that in all cases equality is reached when \(\epsilon = 0\)). Now, recall Chebyshev’s inequality which states that:

$$ \Pr[L-\mathbb{E}[L]\leq k] ≤ \frac{\mathbb{V}ar[L]}{k^2}$$

Therefore,

$$ \Pr[L \leq \frac{2^\ell-1}{2}]=\Pr[L-\mathbb{E}[X]\leq \frac{2^\ell-1}{2} - \mathbb{E}[X]] $$

Using the Chebyshev inequality and our value of the variance as derived above (note that it is valid to use Chebyshev’s inequality since \(\epsilon(2^\ell − 1) \geq 0\) since \(\epsilon > 0\)), we have that:

$$ = \Pr[L-\mathbb{E}[X]\leq \epsilon(2^\ell-1)] \leq \frac{2^\ell-1}{4\epsilon^2(2^\ell-1)^2}$$

$$= \frac{1}{4\epsilon^2(2^\ell-1)}$$

Therefore, we conclude that:

$$\Pr[|\{S\neq\phi: C_{u_S} + R_{u_s+e_i} = m_i\}| \leq \frac{2^\ell - 1}{2} = \Pr[𝐿 \leq \frac{2^\ell-1}{2}]$$

$$= \boxed{\frac{1}{4\epsilon^2(2^\ell-1)}}$$


QED.<br><br><br>


I'm now going to describe a probabilistic procedure \(A\) that has the following behavior:<br>
1) it has random access to a word \(R\) that agrees with \(C = C(m)\) in at least a \(\frac{1}{2}+\epsilon\) fraction of its \(n\) bits, and<br>
2) it runs in time \(poly(k, \epsilon^{-1})\), and<br>
3) with probability at least \(\frac{3}{4}\) it outputs a list of \(L=O(k\epsilon^{-2})\) “candidate messages” \(m_1, m_2, ..., m_L\) that includes the original message \(m\).<br><br>

We can show that it takes surprisingly few bits to describe \((C_{u_S})_{S\neq\phi})\) by noting that the number of bits that would be chosen would be \(2^\ell\) so an appropriate (logarithmic) choice of \(\ell\) would make the number of bits drastically small. Therefore, let \(\ell= \log(k\epsilon^{-2} + 2)\). From this, we define the following probabilistic procedure \(A\):<br><br>

A(input \(R\) = a word that agrees with \(C = C(m)\) in atleast a \(\frac{1}{2}+\epsilon\) fraction of its \(n\) bits (so the first property is satisfied)):<br><br>

Pick \(\ell\) vectors \(v_1, v_2, ..., v_k \in \mathbb{F}_2^k\) independently and uniformly at random. Then, 
$$u_S = \sum_{i\in S}v_i \Rightarrow C_{u_S} = \sum_{i\in S} C_{v_i}$$

(since \(C_w + C_{e_i} = C_{w+e_i}\) as mentioned earlier).<br><br>

The total number of candidate choices for \(C_{v_i}\) is \(L = 2^\ell = 2^{\log k \epsilon^{-2} + 2} = k \epsilon^{-2} + 2 \in O(k\epsilon^{-2})\). Therefore, if we output a candidate message \(m_1\) for every candidate choice \(c_i\) and find a way to get the procedure to output the original message \(m\) (\(\frac{3}{4}\)ths of the time) in a procedure that runs in \(\text{poly}(k, \epsilon^{−1})\), then we are done.

Note here that the probability that \(\sum\limits_{i\in S} c_i + R_{u_S+e_j} = m_j\) is: 

$$ \Pr[L\geq \frac{2^\ell-1}{2}] = 1 - \frac{1}{4\epsilon^2 k\epsilon^{-2}} = 1 - \frac{1}{4k}$$

Taking the union-bound of this probability for all \(k\) yields a union bound of \(\frac{3}{4}\). Therefore, if we append the majority bit (from all the \(S'\)) \(\sum_{i\in S} c_i + R_{u_s+e_j}\) to a
list \(B\) every time we guess the \(i\)’th candidate choice \(c_i\), and just return \(B\) at the very end, there is a probability of \(\frac{3}{4}\) that the original message would have been returned (since the majority of all
the \(S'\) in \(\sum_{i\in S} c_i + R_{u_s+e_j}=m_j\) is true \(\frac{3}{4}\) of the time). Therefore, the second property is satisfied.<br><br>

Therefore, we now need to ensure that this procedure has a run-time of \(\text{poly}(k, \epsilon^{−1})\). To do this, note that the number of candidate choices for \(C_{v_i}\) is \(O(k\epsilon^{-2})\). Since this needs to be done for all \(v_i\) and since there are \(k\) many \(v_i\)‘s (and the comparison procedure only runs in \(O(1)\) with respect to \(k\)), we have that the total run-time is 

$$O(k^2 \epsilon^{-2}) = \text{poly}(k^2, \epsilon^{−2}) = \text{poly}(k,\epsilon^{-1})$$

Therefore, the third property is satisfied.

Therefore, we have constructed a probabilistic procedure \(A\) that satisfies all the above properties. As desired.<br><br>

QED.<br><br><br>


Finally, I'm going to use all of this to prove something called the Goldreich-Levin theorem, which states that for every function family \(f_n: \{0,1\}^n \rightarrow \{0, 1\}^n\):

$$\text{GL}(x,y) = \sum\limits_{i=1}^n x_i y_i$$

is a hard bit for the function family \(f'_n:\{0, 1\}^n \times \{0, 1\}^n \rightarrow \{0, 1\}^{2n}\) defined by \(\mathcal{f}'(x, y) = (f(x), y)\). That is, if there is a circuit family \(\{C_n\}\) of size \(s(n) ≥ n\) that achieves:

$$\Pr\limits_{x,y}[C_n(x,y) = \text{GL}(\mathcal{f}_n'(x,y))] ≥ \frac{1}{2} + \epsilon(n)$$

then there is a circuit family \(\{C'_n\}\) of size \(s'(n)\) that achieves:

$$\Pr\limits_y[C'_n(x, y) = \mathcal{f}'_n(x, y)] \geq \epsilon'(n)$$

with \(s'(n) = (\frac{s(n)}{\epsilon(n)})^\text{O(1)}\) and \(\epsilon'(n) = (\frac{\epsilon(n)}{n})^\text{O(1)}\). <br><br>

Let \(X\) be the class in which for all \(x \in X\), (where \(|X| \geq \frac{\epsilon(n)}{2}|X|\)):

$$\Pr\limits_y[C(x,y)=\text{GL}(f'_n(x,y))]\geq\frac{1}{2}+\frac{\epsilon(n)}{2}$$

We see then that \(\text{GL}(f'_n(x, y))\) is the \(y\)th bit of the codeword \(C(f_n(x))\), and that for all \(x \in X\):

$$C(f_n(x)) = C_n(x, y) \text{ agree on atleast } \frac{1}{2} + \frac{\epsilon(n)}{2} \text{ of its bits.}$$

Then, using the probabilistic procedure \(A\) that we defined earlier, we can assign the random word to be \(C_n\) such that on input \((x, y)\) we would get \((f_n(x), y)\) for all \(x\in X\), and that this would happen \(\frac{3}{4} \times \frac{1}{L}\) of the time, given that \(x\in X\). However, the probability that \(x\in X\) is \(\frac{\epsilon(n)+1}{2}\) (as described above).<br><br>

Therefore, the total probability that \(A\) outputs \((f_n(x), y)\) is (using the fact that the probability of \(2\)
independent events happening is equal to the products of the probabilities of each event):

$$\epsilon'(n) = L\times \frac{3}{4} \times \frac{1}{\frac{\epsilon(n)+1}{2}}$$

$$\epsilon'(n) = O(\frac{\epsilon(n)^2}{n} \times \frac{1}{\frac{\epsilon(n)+1}{2}}$$

$$\epsilon'(n) = (\frac{\epsilon(n)}{n})^\text{O(1)}$$

Therefore, the probability that \(A\) outputs \((f_n(x),y) \geq (\frac{\epsilon(n)}{n})^\text{O(1)}\). Hence, we have that for the probabilistic procedure:

$$\epsilon'(n) = (\frac{\epsilon(n)}{n})^\text{O(1)}$$

To find the size of the circuit family, we can simply construct a circuit for each step in the run-time.

We already know that the run-time of the algorithm would be: \(\frac{1}{\epsilon'(n)} = (\frac{n}{\epsilon(n)})^\text{O(1)}\) since \(\Pr[C'(x, y) = f'_n(x, y)] \geq \epsilon'(n)\). Therefore, since \(s(n)>n\), we could use a circuit of size \(s(n)\) for each query (while \(n\) expecting atmost a quadratic increase in size by using Razborov's method for circuit-value finding). Hence, the associated size of the circuit would be:

$$s'(n) = (n\frac{s(n)}{\epsilon(n)})^\text{O(1)}$$

$$=(\frac{s(n)}{\epsilon(n)}^\text{O(1)}$$

Therefore, if there is a circuit family \(\{C_n\}\) of size \(s(n) \geq n\) that achieves: \(\Pr\limits{x,y}[C_n(x, y) = \text{GL}(f'_n(x, y))] ≥ \frac{1}{2} + \epsilon(n)\), then there is a circuit family \(\{C'_n\}\) of size \(s'(n)\) that achieves: 

$$\Pr\limits_{x,y}[C'_n(x,y)=f_n'(x,y)]\geq \epsilon'(n)$$

with \(s'(n) = (\frac{s(n)}{\epsilon(n)})^\text{O(1)}\) and \(\epsilon'(n) = (\frac{\epsilon(n)}{n})^\text{O(1)}\).<br><br>

This proves the Goldreich-Levin theorem.<br><br>

QED.
</div>
<br>
<button class="collapsible"><strong>List Decoding of the Reed-Solomon Code</strong></button>
  <div class="content">
  
Coming soon!

  </div>

<br>
<button class="collapsible"><strong>List Decoding of the Parvaresh-Vardy Code</strong></button>
  <div class="content">
  
Coming soon!

  </div>


</div>


  </div>


  </div>
  </div>
</div>

<style>
.rectangle {
  height: 8%;
  width: 55%;
  background-color: black;
  margin-left: auto;
  margin-right: auto;
  color: white;
  display: flex;
  justify-content:center;
  align-items: center;
}

    .centertitleph112 {
      display: block;
      margin-left: auto;
      margin-right: auto;
      text-align: center;
    }

</style>
</head>
<div class="footer">
    <a href="#" class="w3-button w3-white w3-padding-large w3-margin-bottom"><img src="../up_arrow.png", style="width: 25px;height:25px";></i>To the top</a><br>
    <a href=https://www.instagram.com/emiletimothy/><img src="Pictures/icons/instagram-icon.png", alt="Instagram", style="width:25px;height:25px";></a>
    <a href=https://www.linkedin.com/in/emiletimothy/><img src="Pictures/icons/linkedin-icon.png", alt="Linkedin", style="width:25px;height:25px";></a>
    <a href=https://orcid.org/my-orcid?orcid=0000-0003-2893-9469https://orcid.org/my-orcid?orcid=0000-0003-2893-9469/><img src="Pictures/icons/orcid-icon.png", alt="Orcid", style="width:25px;height:25px";></a>
    <a href=https://scholar.google.com/citations?user=nUXwVU8AAAAJ&hl=en/><img src="Pictures/icons/googlescholar-icon.png", alt="Google Scholar", style="width:25px;height:25px";></a>
    <a href=https://github.com/emiletimothy/><img src="../Pictures/icons/github-icon.png", alt="Github Icon", style="width:30px;height:29px";></a><br><br>
<p1 style="color: white"><br><strong>© 2023 by Emile Timothy</strong></p1>
</div>

</body>

</html>


<script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("b1-active");
    var content = this.nextElementSibling;
    if (content.style.maxHeight){
      content.style.maxHeight = null;
    } else {
      content.style.maxHeight = content.scrollHeight + "px";
    }
  });
}
</script>



<style>
/* Style the button that is used to open and close the collapsible content */
.collapsible {
  background-color: #000000;
  color: #ffffff;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  outline: 10px white;
  border: none;
  text-align: center;
  outline: none;
  font-size: 15px;
}

.collapsible:after {
  background-color: #000000;
  content: '+'; /* Unicode character for "plus" sign (+) */
  font-size: 13px;
  outline: #000000;
  color: #ffffff;
  float: right;
  margin-left: 5px;
}

.b1-active:after {
  color: #ffffff;
  font-size:10px;
  content: "-"; /* Unicode character for "minus" sign (-) */
}

/* Add a background color to the button if it is clicked on (add the .active class with JS), and when you move the mouse over it (hover) */
.b1-active, .collapsible:hover {
  background-color: #000000;
}

.content {
  padding: 0 18px;
  color: black;
  max-height: 0;
  overflow: hidden;
  transition: max-height 0.2s ease-out;
}
</style>
